{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torchvision import transforms\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "import os\n",
    "import timm\n",
    "from cleanfid import fid\n",
    "import wandb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from typing import List, Dict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/flix/.cache/huggingface/datasets/flix-k___parquet/flix-k--oct-dataset-val1kv3-899ad0f348fd8f48/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b890a6fde34f6d8227f3b85dd0e6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats already exist\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "class LatentDiffusionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 dataset_name, \n",
    "                 image_column, \n",
    "                 caption_column, \n",
    "                 tokenizer, \n",
    "                 resolution, \n",
    "                 center_crop, \n",
    "                 random_flip,\n",
    "                 train_batch_size,\n",
    "                 val_batch_size,\n",
    "                 num_workers,\n",
    "                 latent_file_path_train=None,\n",
    "                 latent_file_path_val=None,\n",
    "                 use_latents=False):\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.image_column = image_column\n",
    "        self.caption_column = caption_column\n",
    "        self.tokenizer = tokenizer\n",
    "        self.resolution = resolution\n",
    "        self.center_crop = center_crop\n",
    "        self.random_flip = random_flip\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.latent_file_path_train = latent_file_path_train\n",
    "        self.latent_file_path_val = latent_file_path_val\n",
    "        self.use_latents = use_latents\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Your dataset download and preparation logic here\n",
    "        # Make sure not to return anything from this method\n",
    "        # as it is called on every GPU during distributed training\n",
    "\n",
    "        self.dataset = load_dataset(\n",
    "            self.dataset_name,\n",
    "        )\n",
    "\n",
    "        if self.use_latents:\n",
    "\n",
    "            class H5PyTorchDataset(Dataset):\n",
    "                def __init__(self, file_path):\n",
    "                    self.file_path = file_path\n",
    "                    with h5py.File(self.file_path, 'r') as f:\n",
    "                        self.latents = f['latents'][:]\n",
    "                        self.labels = f['labels'][:]\n",
    "\n",
    "                def __getitem__(self, index):\n",
    "                    latent = self.latents[index]\n",
    "                    label = self.labels[index]\n",
    "                    if np.isscalar(label):\n",
    "                        label = np.array([label])\n",
    "\n",
    "                    # Convert the data to PyTorch tensors\n",
    "                    latent = torch.from_numpy(latent)\n",
    "                    label = torch.from_numpy(label)\n",
    "\n",
    "                    return {'latents': latent, 'labels': label}\n",
    "\n",
    "                def __len__(self):\n",
    "                    return len(self.latents)\n",
    "\n",
    "            # Usage\n",
    "            h5_pytorch_dataset_train = H5PyTorchDataset(self.latent_file_path_train)\n",
    "            h5_pytorch_dataset_val = H5PyTorchDataset(self.latent_file_path_val)\n",
    "\n",
    "            self.latent_dataset = dict()\n",
    "            self.latent_dataset[\"train\"] = h5_pytorch_dataset_train\n",
    "            self.latent_dataset[\"val\"] = h5_pytorch_dataset_val\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Your dataset splitting and processing logic here\n",
    "        # Store the resulting datasets as instance variables (e.g., self.train_dataset)\n",
    "        # You can access them in the respective dataloader methods\n",
    "\n",
    "        self.train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(self.resolution) if self.center_crop else transforms.RandomCrop(self.resolution),\n",
    "                transforms.RandomHorizontalFlip() if self.random_flip else transforms.Lambda(lambda x: x),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "                # transforms.Normalize([0.19130389392375946, 0.19130389392375946, 0.19130389392375946], [0.1973849982023239, 0.1973849982023239, 0.1973849982023239])\n",
    "            ]\n",
    "        )\n",
    "        self.train_dataset = self.dataset[\"train\"].with_transform(self.preprocess_train)\n",
    "        self.val_dataset = self.dataset[\"val\"].with_transform(self.preprocess_train)\n",
    "\n",
    "        if self.use_latents:\n",
    "            self.train_latent_dataset = self.latent_dataset[\"train\"]\n",
    "            self.val_latent_dataset = self.latent_dataset[\"val\"]\n",
    "\n",
    "\n",
    "        from PIL import Image\n",
    "        import glob\n",
    "        \n",
    "        if not os.path.exists(\"./val_images\"):\n",
    "            os.mkdir(\"./val_images\")\n",
    "\n",
    "        files = glob.glob(\"./val_images/*\")\n",
    "        if len(files) < len(self.val_dataset):\n",
    "            for i in tqdm(range(len(self.val_dataset))):\n",
    "                img = Image.fromarray(np.array(self.val_dataset[i][\"image\"]))\n",
    "                image_name = self.val_dataset[i][\"caption\"]\n",
    "                img.save(f\"./val_images/{image_name}-{i}.jpg\")\n",
    "                \n",
    "        try:\n",
    "            fid.make_custom_stats(\"val\", fdir=\"./val_images/\")\n",
    "        except:\n",
    "            print(\"Stats already exist\")\n",
    "\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # model = timm.create_model('inception_v3', pretrained=True, num_classes=4).to(device)\n",
    "        # model.load_state_dict(torch.load(\"./finetuned_best.pt\"))\n",
    "        # model.eval()\n",
    "        # model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "        # try:\n",
    "        #     fid.make_custom_stats(\"octv3-val\", fdir=\"./val_images/\", model=model, model_name=\"custom\")\n",
    "        # except:\n",
    "        #     print(\"Stats already exist\")\n",
    "        # model.to(\"cpu\")\n",
    "        # torch.cuda.empty_cache()\n",
    "        # del model\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # def precrocess_train_latents(self, examples):\n",
    "    #     latents = [latent for latent in examples[\"latents\"]]\n",
    "    #     return examples\n",
    "\n",
    "    def preprocess_train(self, examples):\n",
    "        images = [image.convert(\"RGB\") for image in examples[self.image_column]]\n",
    "        captions = [caption for caption in examples[self.caption_column]]\n",
    "        examples[\"pixel_values\"] = [self.train_transforms(image) for image in images]\n",
    "        examples[\"caption\"] = [caption for caption in captions]\n",
    "        examples[\"input_ids\"] = self.tokenize_captions(examples)\n",
    "        return examples\n",
    "\n",
    "    def tokenize_captions(self, examples, is_train=True):\n",
    "        captions = []\n",
    "        for caption in examples[self.caption_column]:\n",
    "            if isinstance(caption, str):\n",
    "                captions.append(caption)\n",
    "            elif isinstance(caption, (list, np.ndarray)):\n",
    "                # take a random caption if there are multiple\n",
    "                captions.append(random.choice(caption) if is_train else caption[0])\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Caption column `{self.caption_column}` should contain either strings or lists of strings.\"\n",
    "                )\n",
    "        inputs = self.tokenizer(\n",
    "            captions, max_length=self.tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs.input_ids\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.use_latents:\n",
    "            return DataLoader(\n",
    "                self.train_latent_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=None,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.train_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=self.collate_fn,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.use_latents:\n",
    "            return DataLoader(\n",
    "                self.val_latent_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=None,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.val_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=self.collate_fn,\n",
    "                batch_size=self.val_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "        captions = [example[\"caption\"] for example in examples]\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids, \"captions\": captions}\n",
    "\n",
    "def precompute_latents(vae, text_encoder, dataloader, train=False, classes=[\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]):\n",
    "    # Create a list to store the latents\n",
    "    latents = []\n",
    "    encoder_states = []\n",
    "\n",
    "    # Set the model to eval mode\n",
    "    vae.eval()\n",
    "\n",
    "    # Iterate over the batches\n",
    "    for batch in tqdm(dataloader):\n",
    "        # Extract the pixel values\n",
    "        pixel_values = batch[\"pixel_values\"].to(\"cuda\")\n",
    "\n",
    "\n",
    "        # Encode the pixel values\n",
    "        with torch.no_grad():\n",
    "            latent = vae.encode(pixel_values).latent_dist.sample().cpu().numpy()\n",
    "\n",
    "        class_names = batch[\"captions\"]\n",
    "        # Encode the class names using the index of the class name in the list of classes\n",
    "        class_indices = [classes.index(class_name) for class_name in class_names]\n",
    "\n",
    "        # Append the latents to the list\n",
    "        latents.append(latent)\n",
    "        encoder_states.append(class_indices)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # Combine all latents and labels\n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "    encoder_states = np.concatenate(encoder_states, axis=0)\n",
    "    \n",
    "    # Save the latent representations to disk\n",
    "    mode = 'train' if train else 'test'\n",
    "    \n",
    "    # use h5py highest compression rate\n",
    "\n",
    "\n",
    "\n",
    "    file = h5py.File(f'trained_vae_kl_dv3_{mode}.h5', 'w')\n",
    "\n",
    "    # Save the array to the file\n",
    "    file.create_dataset('latents', data=latents)\n",
    "    file.create_dataset('labels', data=encoder_states)\n",
    "\n",
    "    # Close the file\n",
    "    file.close()\n",
    "\n",
    "    # Save the latent representations to disk\n",
    "    # mode = 'train' if train else 'test'\n",
    "    # np.save(f'{mode}_latents.npy', latents)\n",
    "    # np.save(f'{mode}_encoder_states.npy', encoder_states)\n",
    "\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"tokenizer\")\n",
    "vae = AutoencoderKL.from_pretrained(\"flix-k/custom_model_parts\", subfolder=\"vae_trained_kl\").to(\"cuda\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"text_encoder\").to(\"cuda\")\n",
    "data_module = LatentDiffusionDataModule(dataset_name=\"flix-k/oct-dataset-val1kv3\", \n",
    "                                        image_column=\"image\", \n",
    "                                        caption_column=\"caption\", \n",
    "                                        tokenizer=tokenizer, \n",
    "                                        resolution=512, \n",
    "                                        center_crop=False, \n",
    "                                        random_flip=False,\n",
    "                                        train_batch_size=1,\n",
    "                                        val_batch_size=1,\n",
    "                                        num_workers=0,)\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "train_dataloader = data_module.val_dataloader()\n",
    "# Precompute the latents for the training set\n",
    "# train_latents = precompute_latents(vae, text_encoder, train_dataloader, train=True)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# # show the first 10 images in the dataloader\n",
    "# for i, batch in enumerate(train_dataloader):\n",
    "#     if i == 10:\n",
    "#         break\n",
    "#     plt.imshow(batch['pixel_values'][0].permute(1, 2, 0).cpu().numpy())\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNV']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "for batch in train_dataloader:\n",
    "    print(batch[\"captions\"])\n",
    "    pixel_values = batch[\"pixel_values\"].to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(pixel_values).latent_dist\n",
    "    with open('company_data.pkl', 'wb') as output:\n",
    "        pickle.dump(latent, output, pickle.HIGHEST_PROTOCOL)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "with open('company_data.pkl', 'rb') as inp:\n",
    "    latent = pickle.load(inp)\n",
    "    latent = latent.sample().cpu().numpy()\n",
    "    print(latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [09:51<00:00,  6.77it/s]\n"
     ]
    }
   ],
   "source": [
    "train_latents = precompute_latents(vae, text_encoder, train_dataloader, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 274\u001b[0m\n\u001b[1;32m    262\u001b[0m text_encoder \u001b[39m=\u001b[39m CLIPTextModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mstabilityai/stable-diffusion-2-1-base\u001b[39m\u001b[39m\"\u001b[39m, subfolder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext_encoder\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m data_module \u001b[39m=\u001b[39m LatentDiffusionDataModule(dataset_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mflix-k/oct-dataset-val1kv3\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m    264\u001b[0m                                         image_column\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m    265\u001b[0m                                         caption_column\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcaption\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m                                         val_batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    272\u001b[0m                                         num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,)\n\u001b[0;32m--> 274\u001b[0m data_module\u001b[39m.\u001b[39;49mprepare_data()\n\u001b[1;32m    275\u001b[0m data_module\u001b[39m.\u001b[39msetup()\n\u001b[1;32m    276\u001b[0m train_dataloader \u001b[39m=\u001b[39m data_module\u001b[39m.\u001b[39mtrain_dataloader()\n",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m, in \u001b[0;36mLatentDiffusionDataModule.prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[39m# Your dataset download and preparation logic here\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[39m# Make sure not to return anything from this method\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[39m# as it is called on every GPU during distributed training\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m load_dataset(\n\u001b[1;32m     41\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_name,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     44\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_latents:\n\u001b[1;32m     46\u001b[0m         \u001b[39mclass\u001b[39;00m \u001b[39mH5PyTorchDataset\u001b[39;00m(Dataset):\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/datasets/load.py:1767\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1762\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   1763\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1764\u001b[0m )\n\u001b[1;32m   1766\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1767\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1768\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1769\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1770\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1771\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1772\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1773\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1774\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1775\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1776\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1777\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1778\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   1779\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1780\u001b[0m )\n\u001b[1;32m   1782\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/datasets/load.py:1498\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1497\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1498\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1499\u001b[0m     path,\n\u001b[1;32m   1500\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1501\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1502\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1503\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1504\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1505\u001b[0m )\n\u001b[1;32m   1507\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/datasets/load.py:1192\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1185\u001b[0m             path,\n\u001b[1;32m   1186\u001b[0m             revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m             dynamic_modules_path\u001b[39m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1190\u001b[0m         )\u001b[39m.\u001b[39mget_module()\n\u001b[1;32m   1191\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1192\u001b[0m         \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithoutScript(\n\u001b[1;32m   1193\u001b[0m             path,\n\u001b[1;32m   1194\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1195\u001b[0m             data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1196\u001b[0m             data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1197\u001b[0m             download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1198\u001b[0m             download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1199\u001b[0m         )\u001b[39m.\u001b[39;49mget_module()\n\u001b[1;32m   1200\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[1;32m   1201\u001b[0m     \u001b[39mException\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m ) \u001b[39mas\u001b[39;00m e1:  \u001b[39m# noqa: all the attempts failed, before raising the error we should check if the module is already cached.\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/datasets/load.py:810\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m     download_config\u001b[39m.\u001b[39mdownload_desc \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDownloading metadata\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    809\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m     dataset_infos_path \u001b[39m=\u001b[39m cached_path(\n\u001b[1;32m    811\u001b[0m         hf_hub_url(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, config\u001b[39m.\u001b[39;49mDATASETDICT_INFOS_FILENAME, revision\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrevision),\n\u001b[1;32m    812\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m    813\u001b[0m     )\n\u001b[1;32m    814\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(dataset_infos_path, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    815\u001b[0m         dataset_infos: DatasetInfosDict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/datasets/utils/file_utils.py:183\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    182\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    184\u001b[0m         url_or_filename,\n\u001b[1;32m    185\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    186\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[1;32m    187\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    188\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[1;32m    189\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[1;32m    190\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[1;32m    191\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[1;32m    192\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    193\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_auth_token,\n\u001b[1;32m    194\u001b[0m         ignore_url_params\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mignore_url_params,\n\u001b[1;32m    195\u001b[0m         storage_options\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    196\u001b[0m         download_desc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mdownload_desc,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[1;32m    198\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    199\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/datasets/utils/file_utils.py:509\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, storage_options, download_desc)\u001b[0m\n\u001b[1;32m    507\u001b[0m     connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 509\u001b[0m     response \u001b[39m=\u001b[39m http_head(\n\u001b[1;32m    510\u001b[0m         url,\n\u001b[1;32m    511\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    512\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    513\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m    514\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    515\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    516\u001b[0m     )\n\u001b[1;32m    517\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:  \u001b[39m# ok\u001b[39;00m\n\u001b[1;32m    518\u001b[0m         etag \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mETag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m use_etag \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/datasets/utils/file_utils.py:413\u001b[0m, in \u001b[0;36mhttp_head\u001b[0;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[1;32m    411\u001b[0m headers \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(headers) \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    412\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m get_datasets_user_agent(user_agent\u001b[39m=\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 413\u001b[0m response \u001b[39m=\u001b[39m _request_with_retry(\n\u001b[1;32m    414\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    415\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    416\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    417\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    418\u001b[0m     cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    419\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49mallow_redirects,\n\u001b[1;32m    420\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    421\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    422\u001b[0m )\n\u001b[1;32m    423\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/datasets/utils/file_utils.py:320\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    318\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    319\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(), url\u001b[39m=\u001b[39;49murl, timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    321\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/urllib3/connection.py:414\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    406\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs\n\u001b[1;32m    407\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(context, \u001b[39m\"\u001b[39m\u001b[39mload_default_certs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    411\u001b[0m ):\n\u001b[1;32m    412\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 414\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    415\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    416\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    417\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    418\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    419\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    420\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    421\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    422\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    423\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    424\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    425\u001b[0m )\n\u001b[1;32m    427\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    431\u001b[0m     default_ssl_context\n\u001b[1;32m    432\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock, \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    434\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock\u001b[39m.\u001b[39mversion() \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mTLSv1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTLSv1.1\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m    435\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    437\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m         SNIMissingWarning,\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[39mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39mwrap_socket(sock)\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/ssl.py:500\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    497\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    498\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    501\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    502\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    503\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    504\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    505\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    506\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    507\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    508\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/ssl.py:1040\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m   1038\u001b[0m             \u001b[39m# non-blocking\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1040\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1041\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/xformers/lib/python3.8/ssl.py:1309\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m block:\n\u001b[1;32m   1308\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1309\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1310\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1311\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "class LatentDiffusionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 dataset_name, \n",
    "                 image_column, \n",
    "                 caption_column, \n",
    "                 tokenizer, \n",
    "                 resolution, \n",
    "                 center_crop, \n",
    "                 random_flip,\n",
    "                 train_batch_size,\n",
    "                 val_batch_size,\n",
    "                 num_workers,\n",
    "                 latent_file_path_train=None,\n",
    "                 latent_file_path_val=None,\n",
    "                 use_latents=False):\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.image_column = image_column\n",
    "        self.caption_column = caption_column\n",
    "        self.tokenizer = tokenizer\n",
    "        self.resolution = resolution\n",
    "        self.center_crop = center_crop\n",
    "        self.random_flip = random_flip\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.latent_file_path_train = latent_file_path_train\n",
    "        self.latent_file_path_val = latent_file_path_val\n",
    "        self.use_latents = use_latents\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Your dataset download and preparation logic here\n",
    "        # Make sure not to return anything from this method\n",
    "        # as it is called on every GPU during distributed training\n",
    "\n",
    "        self.dataset = load_dataset(\n",
    "            self.dataset_name,\n",
    "        )\n",
    "\n",
    "        if self.use_latents:\n",
    "\n",
    "            class H5PyTorchDataset(Dataset):\n",
    "                def __init__(self, file_path):\n",
    "                    self.file_path = file_path\n",
    "                    with h5py.File(self.file_path, 'r') as f:\n",
    "                        self.latents = f['latents'][:]\n",
    "                        self.labels = f['labels'][:]\n",
    "\n",
    "                def __getitem__(self, index):\n",
    "                    latent = self.latents[index]\n",
    "                    label = self.labels[index]\n",
    "                    if np.isscalar(label):\n",
    "                        label = np.array([label])\n",
    "\n",
    "                    # Convert the data to PyTorch tensors\n",
    "                    latent = torch.from_numpy(latent)\n",
    "                    label = torch.from_numpy(label)\n",
    "\n",
    "                    return {'latents': latent, 'labels': label}\n",
    "\n",
    "                def __len__(self):\n",
    "                    return len(self.latents)\n",
    "\n",
    "            # Usage\n",
    "            h5_pytorch_dataset_train = H5PyTorchDataset(self.latent_file_path_train)\n",
    "            h5_pytorch_dataset_val = H5PyTorchDataset(self.latent_file_path_val)\n",
    "\n",
    "            self.latent_dataset = dict()\n",
    "            self.latent_dataset[\"train\"] = h5_pytorch_dataset_train\n",
    "            self.latent_dataset[\"val\"] = h5_pytorch_dataset_val\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Your dataset splitting and processing logic here\n",
    "        # Store the resulting datasets as instance variables (e.g., self.train_dataset)\n",
    "        # You can access them in the respective dataloader methods\n",
    "\n",
    "        self.train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(self.resolution) if self.center_crop else transforms.RandomCrop(self.resolution),\n",
    "                transforms.RandomHorizontalFlip() if self.random_flip else transforms.Lambda(lambda x: x),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "                # transforms.Normalize([0.19130389392375946, 0.19130389392375946, 0.19130389392375946], [0.1973849982023239, 0.1973849982023239, 0.1973849982023239])\n",
    "            ]\n",
    "        )\n",
    "        self.train_dataset = self.dataset[\"train\"].with_transform(self.preprocess_train)\n",
    "        self.val_dataset = self.dataset[\"val\"].with_transform(self.preprocess_train)\n",
    "\n",
    "        if self.use_latents:\n",
    "            self.train_latent_dataset = self.latent_dataset[\"train\"]\n",
    "            self.val_latent_dataset = self.latent_dataset[\"val\"]\n",
    "\n",
    "\n",
    "        from PIL import Image\n",
    "        import glob\n",
    "        \n",
    "        if not os.path.exists(\"./val_images\"):\n",
    "            os.mkdir(\"./val_images\")\n",
    "\n",
    "        files = glob.glob(\"./val_images/*\")\n",
    "        if len(files) < len(self.val_dataset):\n",
    "            for i in tqdm(range(len(self.val_dataset))):\n",
    "                img = Image.fromarray(np.array(self.val_dataset[i][\"image\"]))\n",
    "                image_name = self.val_dataset[i][\"caption\"]\n",
    "                img.save(f\"./val_images/{image_name}-{i}.jpg\")\n",
    "                \n",
    "        try:\n",
    "            fid.make_custom_stats(\"val\", fdir=\"./val_images/\")\n",
    "        except:\n",
    "            print(\"Stats already exist\")\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = timm.create_model('inception_v3', pretrained=True, num_classes=4).to(device)\n",
    "        model.load_state_dict(torch.load(\"./finetuned_best.pt\"))\n",
    "        model.eval()\n",
    "        model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "        try:\n",
    "            fid.make_custom_stats(\"octv3-val\", fdir=\"./val_images/\", model=model, model_name=\"custom\")\n",
    "        except:\n",
    "            print(\"Stats already exist\")\n",
    "        model.to(\"cpu\")\n",
    "        torch.cuda.empty_cache()\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # def precrocess_train_latents(self, examples):\n",
    "    #     latents = [latent for latent in examples[\"latents\"]]\n",
    "    #     return examples\n",
    "\n",
    "    def preprocess_train(self, examples):\n",
    "        images = [image.convert(\"RGB\") for image in examples[self.image_column]]\n",
    "        captions = [caption for caption in examples[self.caption_column]]\n",
    "        examples[\"pixel_values\"] = [self.train_transforms(image) for image in images]\n",
    "        examples[\"caption\"] = [caption for caption in captions]\n",
    "        examples[\"input_ids\"] = self.tokenize_captions(examples)\n",
    "        return examples\n",
    "\n",
    "    def tokenize_captions(self, examples, is_train=True):\n",
    "        captions = []\n",
    "        for caption in examples[self.caption_column]:\n",
    "            if isinstance(caption, str):\n",
    "                captions.append(caption)\n",
    "            elif isinstance(caption, (list, np.ndarray)):\n",
    "                # take a random caption if there are multiple\n",
    "                captions.append(random.choice(caption) if is_train else caption[0])\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Caption column `{self.caption_column}` should contain either strings or lists of strings.\"\n",
    "                )\n",
    "        inputs = self.tokenizer(\n",
    "            captions, max_length=self.tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs.input_ids\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.use_latents:\n",
    "            return DataLoader(\n",
    "                self.train_latent_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=None,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.train_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=self.collate_fn,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.use_latents:\n",
    "            return DataLoader(\n",
    "                self.val_latent_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=None,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.val_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=self.collate_fn,\n",
    "                batch_size=self.val_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "        captions = [example[\"caption\"] for example in examples]\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids, \"captions\": captions}\n",
    "\n",
    "def precompute_latents(vae, text_encoder, dataloader, train=False, classes=[\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]):\n",
    "    # Create a list to store the latents\n",
    "    latents = []\n",
    "    encoder_states = []\n",
    "\n",
    "    # Set the model to eval mode\n",
    "    vae.eval()\n",
    "\n",
    "    # Iterate over the batches\n",
    "    for batch in tqdm(dataloader):\n",
    "        # Extract the pixel values\n",
    "        pixel_values = batch[\"pixel_values\"].to(\"cuda\")\n",
    "\n",
    "\n",
    "        # Encode the pixel values\n",
    "        with torch.no_grad():\n",
    "            latent = vae.encode(pixel_values).latent_dist.sample().cpu().numpy()\n",
    "\n",
    "        class_names = batch[\"captions\"]\n",
    "        # Encode the class names using the index of the class name in the list of classes\n",
    "        class_indices = [classes.index(class_name) for class_name in class_names]\n",
    "\n",
    "        # Append the latents to the list\n",
    "        latents.append(latent)\n",
    "        encoder_states.append(class_indices)\n",
    "\n",
    "        \n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "class LatentDiffusionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 dataset_name, \n",
    "                 image_column, \n",
    "                 caption_column, \n",
    "                 tokenizer, \n",
    "                 resolution, \n",
    "                 center_crop, \n",
    "                 random_flip,\n",
    "                 train_batch_size,\n",
    "                 val_batch_size,\n",
    "                 num_workers,\n",
    "                 latent_file_path_train=None,\n",
    "                 latent_file_path_val=None,\n",
    "                 use_latents=False):\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.image_column = image_column\n",
    "        self.caption_column = caption_column\n",
    "        self.tokenizer = tokenizer\n",
    "        self.resolution = resolution\n",
    "        self.center_crop = center_crop\n",
    "        self.random_flip = random_flip\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.latent_file_path_train = latent_file_path_train\n",
    "        self.latent_file_path_val = latent_file_path_val\n",
    "        self.use_latents = use_latents\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Your dataset download and preparation logic here\n",
    "        # Make sure not to return anything from this method\n",
    "        # as it is called on every GPU during distributed training\n",
    "\n",
    "        self.dataset = load_dataset(\n",
    "            self.dataset_name,\n",
    "        )\n",
    "\n",
    "        if self.use_latents:\n",
    "\n",
    "            class H5PyTorchDataset(Dataset):\n",
    "                def __init__(self, file_path):\n",
    "                    self.file_path = file_path\n",
    "                    with h5py.File(self.file_path, 'r') as f:\n",
    "                        self.latents = f['latents'][:]\n",
    "                        self.labels = f['labels'][:]\n",
    "\n",
    "                def __getitem__(self, index):\n",
    "                    latent = self.latents[index]\n",
    "                    label = self.labels[index]\n",
    "                    if np.isscalar(label):\n",
    "                        label = np.array([label])\n",
    "\n",
    "                    # Convert the data to PyTorch tensors\n",
    "                    latent = torch.from_numpy(latent)\n",
    "                    label = torch.from_numpy(label)\n",
    "\n",
    "                    return {'latents': latent, 'labels': label}\n",
    "\n",
    "                def __len__(self):\n",
    "                    return len(self.latents)\n",
    "\n",
    "            # Usage\n",
    "            h5_pytorch_dataset_train = H5PyTorchDataset(self.latent_file_path_train)\n",
    "            h5_pytorch_dataset_val = H5PyTorchDataset(self.latent_file_path_val)\n",
    "\n",
    "            self.latent_dataset = dict()\n",
    "            self.latent_dataset[\"train\"] = h5_pytorch_dataset_train\n",
    "            self.latent_dataset[\"val\"] = h5_pytorch_dataset_val\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Your dataset splitting and processing logic here\n",
    "        # Store the resulting datasets as instance variables (e.g., self.train_dataset)\n",
    "        # You can access them in the respective dataloader methods\n",
    "\n",
    "        self.train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(self.resolution) if self.center_crop else transforms.RandomCrop(self.resolution),\n",
    "                transforms.RandomHorizontalFlip() if self.random_flip else transforms.Lambda(lambda x: x),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "                # transforms.Normalize([0.19130389392375946, 0.19130389392375946, 0.19130389392375946], [0.1973849982023239, 0.1973849982023239, 0.1973849982023239])\n",
    "            ]\n",
    "        )\n",
    "        self.train_dataset = self.dataset[\"train\"].with_transform(self.preprocess_train)\n",
    "        self.val_dataset = self.dataset[\"val\"].with_transform(self.preprocess_train)\n",
    "\n",
    "        if self.use_latents:\n",
    "            self.train_latent_dataset = self.latent_dataset[\"train\"]\n",
    "            self.val_latent_dataset = self.latent_dataset[\"val\"]\n",
    "\n",
    "\n",
    "        from PIL import Image\n",
    "        import glob\n",
    "        \n",
    "        if not os.path.exists(\"./val_images\"):\n",
    "            os.mkdir(\"./val_images\")\n",
    "\n",
    "        files = glob.glob(\"./val_images/*\")\n",
    "        if len(files) < len(self.val_dataset):\n",
    "            for i in tqdm(range(len(self.val_dataset))):\n",
    "                img = Image.fromarray(np.array(self.val_dataset[i][\"image\"]))\n",
    "                image_name = self.val_dataset[i][\"caption\"]\n",
    "                img.save(f\"./val_images/{image_name}-{i}.jpg\")\n",
    "                \n",
    "        try:\n",
    "            fid.make_custom_stats(\"val\", fdir=\"./val_images/\")\n",
    "        except:\n",
    "            print(\"Stats already exist\")\n",
    "\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # model = timm.create_model('inception_v3', pretrained=True, num_classes=4).to(device)\n",
    "        # model.load_state_dict(torch.load(\"./finetuned_best.pt\"))\n",
    "        # model.eval()\n",
    "        # model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "        # try:\n",
    "        #     fid.make_custom_stats(\"octv3-val\", fdir=\"./val_images/\", model=model, model_name=\"custom\")\n",
    "        # except:\n",
    "        #     print(\"Stats already exist\")\n",
    "        # model.to(\"cpu\")\n",
    "        # torch.cuda.empty_cache()\n",
    "        # del model\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # def precrocess_train_latents(self, examples):\n",
    "    #     latents = [latent for latent in examples[\"latents\"]]\n",
    "    #     return examples\n",
    "\n",
    "    def preprocess_train(self, examples):\n",
    "        images = [image.convert(\"RGB\") for image in examples[self.image_column]]\n",
    "        captions = [caption for caption in examples[self.caption_column]]\n",
    "        examples[\"pixel_values\"] = [self.train_transforms(image) for image in images]\n",
    "        examples[\"caption\"] = [caption for caption in captions]\n",
    "        examples[\"input_ids\"] = self.tokenize_captions(examples)\n",
    "        return examples\n",
    "\n",
    "    def tokenize_captions(self, examples, is_train=True):\n",
    "        captions = []\n",
    "        for caption in examples[self.caption_column]:\n",
    "            if isinstance(caption, str):\n",
    "                captions.append(caption)\n",
    "            elif isinstance(caption, (list, np.ndarray)):\n",
    "                # take a random caption if there are multiple\n",
    "                captions.append(random.choice(caption) if is_train else caption[0])\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Caption column `{self.caption_column}` should contain either strings or lists of strings.\"\n",
    "                )\n",
    "        inputs = self.tokenizer(\n",
    "            captions, max_length=self.tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs.input_ids\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.use_latents:\n",
    "            return DataLoader(\n",
    "                self.train_latent_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=None,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.train_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=self.collate_fn,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.use_latents:\n",
    "            return DataLoader(\n",
    "                self.val_latent_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=None,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.val_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=self.collate_fn,\n",
    "                batch_size=self.val_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "        captions = [example[\"caption\"] for example in examples]\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids, \"captions\": captions}\n",
    "\n",
    "def precompute_latents(vae, text_encoder, dataloader, train=False, classes=[\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]):\n",
    "    # Create a list to store the latents\n",
    "    latents = []\n",
    "    encoder_states = []\n",
    "\n",
    "    # Set the model to eval mode\n",
    "    vae.eval()\n",
    "\n",
    "    # Iterate over the batches\n",
    "    for batch in tqdm(dataloader):\n",
    "        # Extract the pixel values\n",
    "        pixel_values = batch[\"pixel_values\"].to(\"cuda\")\n",
    "\n",
    "\n",
    "        # Encode the pixel values\n",
    "        with torch.no_grad():\n",
    "            latent = vae.encode(pixel_values).latent_dist.sample().cpu().numpy()\n",
    "\n",
    "        class_names = batch[\"captions\"]\n",
    "        # Encode the class names using the index of the class name in the list of classes\n",
    "        class_indices = [classes.index(class_name) for class_name in class_names]\n",
    "\n",
    "        # Append the latents to the list\n",
    "        latents.append(latent)\n",
    "        encoder_states.append(class_indices)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # Combine all latents and labels\n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "    encoder_states = np.concatenate(encoder_states, axis=0)\n",
    "    \n",
    "    # Save the latent representations to disk\n",
    "    mode = 'train' if train else 'test'\n",
    "    \n",
    "    # use h5py highest compression rate\n",
    "\n",
    "\n",
    "\n",
    "    file = h5py.File(f'trained_vae_kl_dv3_{mode}.h5', 'w')\n",
    "\n",
    "    # Save the array to the file\n",
    "    file.create_dataset('latents', data=latents)\n",
    "    file.create_dataset('labels', data=encoder_states)\n",
    "\n",
    "    # Close the file\n",
    "    file.close()\n",
    "\n",
    "    # Save the latent representations to disk\n",
    "    # mode = 'train' if train else 'test'\n",
    "    # np.save(f'{mode}_latents.npy', latents)\n",
    "    # np.save(f'{mode}_encoder_states.npy', encoder_states)\n",
    "\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"tokenizer\")\n",
    "vae = AutoencoderKL.from_pretrained(\"flix-k/custom_model_parts\", subfolder=\"vae_trained_kl\").to(\"cuda\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"text_encoder\").to(\"cuda\")\n",
    "data_module = LatentDiffusionDataModule(dataset_name=\"flix-k/oct-dataset-val1kv3\", \n",
    "                                        image_column=\"image\", \n",
    "                                        caption_column=\"caption\", \n",
    "                                        tokenizer=tokenizer, \n",
    "                                        resolution=512, \n",
    "                                        center_crop=False, \n",
    "                                        random_flip=False,\n",
    "                                        train_batch_size=1,\n",
    "                                        val_batch_size=1,\n",
    "                                        num_workers=0,)\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "train_dataloader = data_module.val_dataloader()\n",
    "\n",
    "    # Combine all latents and labels\n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "    encoder_states = np.concatenate(encoder_states, axis=0)\n",
    "    \n",
    "    # Save the latent representations to disk\n",
    "    mode = 'train' if train else 'test'\n",
    "    \n",
    "    # use h5py highest compression rate\n",
    "\n",
    "\n",
    "\n",
    "    file = h5py.File(f'trained_vae_dv3_{mode}.h5', 'w')\n",
    "\n",
    "    # Save the array to the file\n",
    "    file.create_dataset('latents', data=latents)\n",
    "    file.create_dataset('labels', data=encoder_states)\n",
    "\n",
    "    # Close the file\n",
    "    file.close()\n",
    "\n",
    "    # Save the latent representations to disk\n",
    "    # mode = 'train' if train else 'test'\n",
    "    # np.save(f'{mode}_latents.npy', latents)\n",
    "    # np.save(f'{mode}_encoder_states.npy', encoder_states)\n",
    "\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"tokenizer\")\n",
    "vae = AutoencoderKL.from_pretrained(\"flix-k/custom_model_parts\", subfolder=\"vae_trained\").to(\"cuda\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"text_encoder\").to(\"cuda\")\n",
    "data_module = LatentDiffusionDataModule(dataset_name=\"flix-k/oct-dataset-val1kv3\", \n",
    "                                        image_column=\"image\", \n",
    "                                        caption_column=\"caption\", \n",
    "                                        tokenizer=tokenizer, \n",
    "                                        resolution=512, \n",
    "                                        center_crop=False, \n",
    "                                        random_flip=False,\n",
    "                                        train_batch_size=1,\n",
    "                                        val_batch_size=1,\n",
    "                                        num_workers=0,)\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "# Precompute the latents for the training set\n",
    "# train_latents = precompute_latents(vae, text_encoder, train_dataloader, train=True)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# # show the first 10 images in the dataloader\n",
    "# for i, batch in enumerate(train_dataloader):\n",
    "#     if i == 10:\n",
    "#         break\n",
    "#     plt.imshow(batch['pixel_values'][0].permute(1, 2, 0).cpu().numpy())\n",
    "#     plt.show()\n",
    "train_latents = precompute_latents(vae, text_encoder, train_dataloader, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 1024])\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "string = \"DRUSEN\"\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"text_encoder\").to(\"cuda\")\n",
    "\n",
    "\n",
    "# inputs = tokenizer(\n",
    "#     string, max_length=4 , padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "# )\n",
    "\n",
    "inputs = tokenizer(\n",
    "    string, max_length=tokenizer.model_max_length , padding=\"do_not_pad\", truncation=True, return_tensors=\"pt\"\n",
    ").input_ids\n",
    "\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Encode the string using the CLIPTextModel\n",
    "with torch.no_grad():\n",
    "    encoded = text_encoder(inputs)[0]\n",
    "\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"text_encoder\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 output = text_encoder(**<span style=\"color: #808000; text-decoration-color: #808000\">\"NORMAL\"</span>)                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(output.shape)                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>CLIPTextModel object argument after ** must be a mapping, not str\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 output = text_encoder(**\u001b[33m\"\u001b[0m\u001b[33mNORMAL\u001b[0m\u001b[33m\"\u001b[0m)                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[96mprint\u001b[0m(output.shape)                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0mCLIPTextModel object argument after ** must be a mapping, not str\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "string = \"NORMAL\"\n",
    "inputs = tokenizer(\n",
    "    string, max_length=tokenizer.model_max_length , padding=\"do_not_pad\", truncation=True, return_tensors=\"pt\"\n",
    ").input_ids\n",
    "\n",
    "output = text_encoder(inputs)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"flix-k/custom_model_parts\", subfolder=\"vae_trained_kl\").to(\"cuda\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
