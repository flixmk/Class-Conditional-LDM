{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.0+cu118 with CUDA 1108 (you have 1.13.0+cu116)\n",
      "    Python  3.8.16 (you have 3.8.10)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "2023-05-26 12:03:07.623886: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-26 12:03:07.706518: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-26 12:03:08.093441: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n",
      "2023-05-26 12:03:08.093490: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n",
      "2023-05-26 12:03:08.093493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torchvision import transforms\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "import os\n",
    "import timm\n",
    "from cleanfid import fid\n",
    "import wandb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from typing import List, Dict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/flix/.cache/huggingface/datasets/flix-k___parquet/flix-k--oct-dataset-val1kv3-899ad0f348fd8f48/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae12af0197e4c02b687b542bac4fb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:22<00:00, 178.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats already exist\n",
      "Stats already exist\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "class LatentDiffusionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 dataset_name, \n",
    "                 image_column, \n",
    "                 caption_column, \n",
    "                 tokenizer, \n",
    "                 resolution, \n",
    "                 center_crop, \n",
    "                 random_flip,\n",
    "                 train_batch_size,\n",
    "                 val_batch_size,\n",
    "                 num_workers,\n",
    "                 latent_file_path_train=None,\n",
    "                 latent_file_path_val=None,\n",
    "                 use_latents=False):\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.image_column = image_column\n",
    "        self.caption_column = caption_column\n",
    "        self.tokenizer = tokenizer\n",
    "        self.resolution = resolution\n",
    "        self.center_crop = center_crop\n",
    "        self.random_flip = random_flip\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.latent_file_path_train = latent_file_path_train\n",
    "        self.latent_file_path_val = latent_file_path_val\n",
    "        self.use_latents = use_latents\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Your dataset download and preparation logic here\n",
    "        # Make sure not to return anything from this method\n",
    "        # as it is called on every GPU during distributed training\n",
    "\n",
    "        self.dataset = load_dataset(\n",
    "            self.dataset_name,\n",
    "        )\n",
    "\n",
    "        if self.use_latents:\n",
    "\n",
    "            class H5PyTorchDataset(Dataset):\n",
    "                def __init__(self, file_path):\n",
    "                    self.file_path = file_path\n",
    "                    with h5py.File(self.file_path, 'r') as f:\n",
    "                        self.latents = f['latents'][:]\n",
    "                        self.labels = f['labels'][:]\n",
    "\n",
    "                def __getitem__(self, index):\n",
    "                    latent = self.latents[index]\n",
    "                    label = self.labels[index]\n",
    "                    if np.isscalar(label):\n",
    "                        label = np.array([label])\n",
    "\n",
    "                    # Convert the data to PyTorch tensors\n",
    "                    latent = torch.from_numpy(latent)\n",
    "                    label = torch.from_numpy(label)\n",
    "\n",
    "                    return {'latents': latent, 'labels': label}\n",
    "\n",
    "                def __len__(self):\n",
    "                    return len(self.latents)\n",
    "\n",
    "            # Usage\n",
    "            h5_pytorch_dataset_train = H5PyTorchDataset(self.latent_file_path_train)\n",
    "            h5_pytorch_dataset_val = H5PyTorchDataset(self.latent_file_path_val)\n",
    "\n",
    "            self.latent_dataset = dict()\n",
    "            self.latent_dataset[\"train\"] = h5_pytorch_dataset_train\n",
    "            self.latent_dataset[\"val\"] = h5_pytorch_dataset_val\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Your dataset splitting and processing logic here\n",
    "        # Store the resulting datasets as instance variables (e.g., self.train_dataset)\n",
    "        # You can access them in the respective dataloader methods\n",
    "\n",
    "        self.train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(self.resolution) if self.center_crop else transforms.RandomCrop(self.resolution),\n",
    "                transforms.RandomHorizontalFlip() if self.random_flip else transforms.Lambda(lambda x: x),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "                # transforms.Normalize([0.19130389392375946, 0.19130389392375946, 0.19130389392375946], [0.1973849982023239, 0.1973849982023239, 0.1973849982023239])\n",
    "            ]\n",
    "        )\n",
    "        self.train_dataset = self.dataset[\"train\"].with_transform(self.preprocess_train)\n",
    "        self.val_dataset = self.dataset[\"val\"].with_transform(self.preprocess_train)\n",
    "\n",
    "        if self.use_latents:\n",
    "            self.train_latent_dataset = self.latent_dataset[\"train\"]\n",
    "            self.val_latent_dataset = self.latent_dataset[\"val\"]\n",
    "\n",
    "\n",
    "        from PIL import Image\n",
    "        import glob\n",
    "        \n",
    "        if not os.path.exists(\"./val_images\"):\n",
    "            os.mkdir(\"./val_images\")\n",
    "\n",
    "        files = glob.glob(\"./val_images/*\")\n",
    "        if len(files) < len(self.val_dataset):\n",
    "            for i in tqdm(range(len(self.val_dataset))):\n",
    "                img = Image.fromarray(np.array(self.val_dataset[i][\"image\"]))\n",
    "                image_name = self.val_dataset[i][\"caption\"]\n",
    "                img.save(f\"./val_images/{image_name}-{i}.jpg\")\n",
    "                \n",
    "        try:\n",
    "            fid.make_custom_stats(\"val\", fdir=\"./val_images/\")\n",
    "        except:\n",
    "            print(\"Stats already exist\")\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = timm.create_model('inception_v3', pretrained=True, num_classes=4).to(device)\n",
    "        model.load_state_dict(torch.load(\"./finetuned_best.pt\"))\n",
    "        model.eval()\n",
    "        model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "        try:\n",
    "            fid.make_custom_stats(\"octv3-val\", fdir=\"./val_images/\", model=model, model_name=\"custom\")\n",
    "        except:\n",
    "            print(\"Stats already exist\")\n",
    "        model.to(\"cpu\")\n",
    "        torch.cuda.empty_cache()\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # def precrocess_train_latents(self, examples):\n",
    "    #     latents = [latent for latent in examples[\"latents\"]]\n",
    "    #     return examples\n",
    "\n",
    "    def preprocess_train(self, examples):\n",
    "        images = [image.convert(\"RGB\") for image in examples[self.image_column]]\n",
    "        captions = [caption for caption in examples[self.caption_column]]\n",
    "        examples[\"pixel_values\"] = [self.train_transforms(image) for image in images]\n",
    "        examples[\"caption\"] = [caption for caption in captions]\n",
    "        examples[\"input_ids\"] = self.tokenize_captions(examples)\n",
    "        return examples\n",
    "\n",
    "    def tokenize_captions(self, examples, is_train=True):\n",
    "        captions = []\n",
    "        for caption in examples[self.caption_column]:\n",
    "            if isinstance(caption, str):\n",
    "                captions.append(caption)\n",
    "            elif isinstance(caption, (list, np.ndarray)):\n",
    "                # take a random caption if there are multiple\n",
    "                captions.append(random.choice(caption) if is_train else caption[0])\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Caption column `{self.caption_column}` should contain either strings or lists of strings.\"\n",
    "                )\n",
    "        inputs = self.tokenizer(\n",
    "            captions, max_length=self.tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs.input_ids\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.use_latents:\n",
    "            return DataLoader(\n",
    "                self.train_latent_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=None,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.train_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=self.collate_fn,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.use_latents:\n",
    "            return DataLoader(\n",
    "                self.val_latent_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=None,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.val_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=self.collate_fn,\n",
    "                batch_size=self.val_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "        captions = [example[\"caption\"] for example in examples]\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids, \"captions\": captions}\n",
    "\n",
    "def precompute_latents(vae, text_encoder, dataloader, train=False, classes=[\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]):\n",
    "    # Create a list to store the latents\n",
    "    latents = []\n",
    "    encoder_states = []\n",
    "\n",
    "    # Set the model to eval mode\n",
    "    vae.eval()\n",
    "\n",
    "    # Iterate over the batches\n",
    "    for batch in tqdm(dataloader):\n",
    "        # Extract the pixel values\n",
    "        pixel_values = batch[\"pixel_values\"].to(\"cuda\")\n",
    "\n",
    "\n",
    "        # Encode the pixel values\n",
    "        with torch.no_grad():\n",
    "            latent = vae.encode(pixel_values).latent_dist.sample().cpu().numpy()\n",
    "\n",
    "        class_names = batch[\"captions\"]\n",
    "        # Encode the class names using the index of the class name in the list of classes\n",
    "        class_indices = [classes.index(class_name) for class_name in class_names]\n",
    "\n",
    "        # Append the latents to the list\n",
    "        latents.append(latent)\n",
    "        encoder_states.append(class_indices)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # Combine all latents and labels\n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "    encoder_states = np.concatenate(encoder_states, axis=0)\n",
    "    \n",
    "    # Save the latent representations to disk\n",
    "    mode = 'train' if train else 'test'\n",
    "    \n",
    "    # use h5py highest compression rate\n",
    "\n",
    "\n",
    "\n",
    "    file = h5py.File(f'trained_vae_kl_dv3_{mode}.h5', 'w')\n",
    "\n",
    "    # Save the array to the file\n",
    "    file.create_dataset('latents', data=latents)\n",
    "    file.create_dataset('labels', data=encoder_states)\n",
    "\n",
    "    # Close the file\n",
    "    file.close()\n",
    "\n",
    "    # Save the latent representations to disk\n",
    "    # mode = 'train' if train else 'test'\n",
    "    # np.save(f'{mode}_latents.npy', latents)\n",
    "    # np.save(f'{mode}_encoder_states.npy', encoder_states)\n",
    "\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"tokenizer\")\n",
    "vae = AutoencoderKL.from_pretrained(\"flix-k/custom_model_parts\", subfolder=\"vae_trained_kl\").to(\"cuda\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"text_encoder\").to(\"cuda\")\n",
    "data_module = LatentDiffusionDataModule(dataset_name=\"flix-k/oct-dataset-val1kv3\", \n",
    "                                        image_column=\"image\", \n",
    "                                        caption_column=\"caption\", \n",
    "                                        tokenizer=tokenizer, \n",
    "                                        resolution=512, \n",
    "                                        center_crop=False, \n",
    "                                        random_flip=False,\n",
    "                                        train_batch_size=1,\n",
    "                                        val_batch_size=1,\n",
    "                                        num_workers=0,)\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "train_dataloader = data_module.val_dataloader()\n",
    "# Precompute the latents for the training set\n",
    "# train_latents = precompute_latents(vae, text_encoder, train_dataloader, train=True)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# # show the first 10 images in the dataloader\n",
    "# for i, batch in enumerate(train_dataloader):\n",
    "#     if i == 10:\n",
    "#         break\n",
    "#     plt.imshow(batch['pixel_values'][0].permute(1, 2, 0).cpu().numpy())\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [09:51<00:00,  6.77it/s]\n"
     ]
    }
   ],
   "source": [
    "train_latents = precompute_latents(vae, text_encoder, train_dataloader, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/flix/.cache/huggingface/datasets/flix-k___parquet/flix-k--oct-dataset-val1kv3-899ad0f348fd8f48/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f048b9b46b7b4c9cb47d4cca86ec51d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats already exist\n",
      "Stats already exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96712/96712 [3:47:47<00:00,  7.08it/s]  \n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "class LatentDiffusionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 dataset_name, \n",
    "                 image_column, \n",
    "                 caption_column, \n",
    "                 tokenizer, \n",
    "                 resolution, \n",
    "                 center_crop, \n",
    "                 random_flip,\n",
    "                 train_batch_size,\n",
    "                 val_batch_size,\n",
    "                 num_workers,\n",
    "                 latent_file_path_train=None,\n",
    "                 latent_file_path_val=None,\n",
    "                 use_latents=False):\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.image_column = image_column\n",
    "        self.caption_column = caption_column\n",
    "        self.tokenizer = tokenizer\n",
    "        self.resolution = resolution\n",
    "        self.center_crop = center_crop\n",
    "        self.random_flip = random_flip\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.latent_file_path_train = latent_file_path_train\n",
    "        self.latent_file_path_val = latent_file_path_val\n",
    "        self.use_latents = use_latents\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Your dataset download and preparation logic here\n",
    "        # Make sure not to return anything from this method\n",
    "        # as it is called on every GPU during distributed training\n",
    "\n",
    "        self.dataset = load_dataset(\n",
    "            self.dataset_name,\n",
    "        )\n",
    "\n",
    "        if self.use_latents:\n",
    "\n",
    "            class H5PyTorchDataset(Dataset):\n",
    "                def __init__(self, file_path):\n",
    "                    self.file_path = file_path\n",
    "                    with h5py.File(self.file_path, 'r') as f:\n",
    "                        self.latents = f['latents'][:]\n",
    "                        self.labels = f['labels'][:]\n",
    "\n",
    "                def __getitem__(self, index):\n",
    "                    latent = self.latents[index]\n",
    "                    label = self.labels[index]\n",
    "                    if np.isscalar(label):\n",
    "                        label = np.array([label])\n",
    "\n",
    "                    # Convert the data to PyTorch tensors\n",
    "                    latent = torch.from_numpy(latent)\n",
    "                    label = torch.from_numpy(label)\n",
    "\n",
    "                    return {'latents': latent, 'labels': label}\n",
    "\n",
    "                def __len__(self):flix-k/oct-dataset-val1kv3-CNV\n",
    "                    return len(self.latents)\n",
    "\n",
    "            # Usage\n",
    "            h5_pytorch_dataset_train = H5PyTorchDataset(self.latent_file_path_train)\n",
    "            h5_pytorch_dataset_val = H5PyTorchDataset(self.latent_file_path_val)\n",
    "\n",
    "            self.latent_dataset = dict()\n",
    "            self.latent_dataset[\"train\"] = h5_pytorch_dataset_train\n",
    "            self.latent_dataset[\"val\"] = h5_pytorch_dataset_val\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Your dataset splitting and processing logic here\n",
    "        # Store the resulting datasets as instance variables (e.g., self.train_dataset)\n",
    "        # You can access them in the respective dataloader methods\n",
    "\n",
    "        self.train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(self.resolution) if self.center_crop else transforms.RandomCrop(self.resolution),\n",
    "                transforms.RandomHorizontalFlip() if self.random_flip else transforms.Lambda(lambda x: x),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "                # transforms.Normalize([0.19130389392375946, 0.19130389392375946, 0.19130389392375946], [0.1973849982023239, 0.1973849982023239, 0.1973849982023239])\n",
    "            ]\n",
    "        )\n",
    "        self.train_dataset = self.dataset[\"train\"].with_transform(self.preprocess_train)\n",
    "        self.val_dataset = self.dataset[\"val\"].with_transform(self.preprocess_train)\n",
    "\n",
    "        if self.use_latents:\n",
    "            self.train_latent_dataset = self.latent_dataset[\"train\"]\n",
    "            self.val_latent_dataset = self.latent_dataset[\"val\"]\n",
    "\n",
    "\n",
    "        from PIL import Image\n",
    "        import glob\n",
    "        \n",
    "        if not os.path.exists(\"./val_images\"):\n",
    "            os.mkdir(\"./val_images\")\n",
    "\n",
    "        files = glob.glob(\"./val_images/*\")\n",
    "        if len(files) < len(self.val_dataset):\n",
    "            for i in tqdm(range(len(self.val_dataset))):\n",
    "                img = Image.fromarray(np.array(self.val_dataset[i][\"image\"]))\n",
    "                image_name = self.val_dataset[i][\"caption\"]\n",
    "                img.save(f\"./val_images/{image_name}-{i}.jpg\")\n",
    "                \n",
    "        try:\n",
    "            fid.make_custom_stats(\"val\", fdir=\"./val_images/\")\n",
    "        except:\n",
    "            print(\"Stats already exist\")\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = timm.create_model('inception_v3', pretrained=True, num_classes=4).to(device)\n",
    "        model.load_state_dict(torch.load(\"./finetuned_best.pt\"))\n",
    "        model.eval()\n",
    "        model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "        try:\n",
    "            fid.make_custom_stats(\"octv3-val\", fdir=\"./val_images/\", model=model, model_name=\"custom\")\n",
    "        except:\n",
    "            print(\"Stats already exist\")\n",
    "        model.to(\"cpu\")\n",
    "        torch.cuda.empty_cache()\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # def precrocess_train_latents(self, examples):\n",
    "    #     latents = [latent for latent in examples[\"latents\"]]\n",
    "    #     return examples\n",
    "\n",
    "    def preprocess_train(self, examples):\n",
    "        images = [image.convert(\"RGB\") for image in examples[self.image_column]]\n",
    "        captions = [caption for caption in examples[self.caption_column]]\n",
    "        examples[\"pixel_values\"] = [self.train_transforms(image) for image in images]\n",
    "        examples[\"caption\"] = [caption for caption in captions]\n",
    "        examples[\"input_ids\"] = self.tokenize_captions(examples)\n",
    "        return examples\n",
    "\n",
    "    def tokenize_captions(self, examples, is_train=True):\n",
    "        captions = []\n",
    "        for caption in examples[self.caption_column]:\n",
    "            if isinstance(caption, str):\n",
    "                captions.append(caption)\n",
    "            elif isinstance(caption, (list, np.ndarray)):\n",
    "                # take a random caption if there are multiple\n",
    "                captions.append(random.choice(caption) if is_train else caption[0])\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Caption column `{self.caption_column}` should contain either strings or lists of strings.\"\n",
    "                )\n",
    "        inputs = self.tokenizer(\n",
    "            captions, max_length=self.tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs.input_ids\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.use_latents:\n",
    "            return DataLoader(\n",
    "                self.train_latent_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=None,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.train_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=self.collate_fn,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.use_latents:\n",
    "            return DataLoader(\n",
    "                self.val_latent_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=None,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.val_dataset,\n",
    "                shuffle=True,\n",
    "                collate_fn=self.collate_fn,\n",
    "                batch_size=self.val_batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "        captions = [example[\"caption\"] for example in examples]\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids, \"captions\": captions}\n",
    "\n",
    "def precompute_latents(vae, text_encoder, dataloader, train=False, classes=[\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]):\n",
    "    # Create a list to store the latents\n",
    "    latents = []\n",
    "    encoder_states = []\n",
    "\n",
    "    # Set the model to eval mode\n",
    "    vae.eval()\n",
    "\n",
    "    # Iterate over the batches\n",
    "    for batch in tqdm(dataloader):\n",
    "        # Extract the pixel values\n",
    "        pixel_values = batch[\"pixel_values\"].to(\"cuda\")\n",
    "\n",
    "\n",
    "        # Encode the pixel values\n",
    "        with torch.no_grad():\n",
    "            latent = vae.encode(pixel_values).latent_dist.sample().cpu().numpy()\n",
    "\n",
    "        class_names = batch[\"captions\"]\n",
    "        # Encode the class names using the index of the class name in the list of classes\n",
    "        class_indices = [classes.index(class_name) for class_name in class_names]\n",
    "\n",
    "        # Append the latents to the list\n",
    "        latents.append(latent)\n",
    "        encoder_states.append(class_indices)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # Combine all latents and labels\n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "    encoder_states = np.concatenate(encoder_states, axis=0)\n",
    "    \n",
    "    # Save the latent representations to disk\n",
    "    mode = 'train' if train else 'test'\n",
    "    \n",
    "    # use h5py highest compression rate\n",
    "\n",
    "\n",
    "\n",
    "    file = h5py.File(f'trained_vae_dv3_{mode}.h5', 'w')\n",
    "\n",
    "    # Save the array to the file\n",
    "    file.create_dataset('latents', data=latents)\n",
    "    file.create_dataset('labels', data=encoder_states)\n",
    "\n",
    "    # Close the file\n",
    "    file.close()\n",
    "\n",
    "    # Save the latent representations to disk\n",
    "    # mode = 'train' if train else 'test'\n",
    "    # np.save(f'{mode}_latents.npy', latents)\n",
    "    # np.save(f'{mode}_encoder_states.npy', encoder_states)\n",
    "\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"tokenizer\")\n",
    "vae = AutoencoderKL.from_pretrained(\"flix-k/custom_model_parts\", subfolder=\"vae_trained\").to(\"cuda\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"text_encoder\").to(\"cuda\")\n",
    "data_module = LatentDiffusionDataModule(dataset_name=\"flix-k/oct-dataset-val1kv3\", \n",
    "                                        image_column=\"image\", \n",
    "                                        caption_column=\"caption\", \n",
    "                                        tokenizer=tokenizer, \n",
    "                                        resolution=512, \n",
    "                                        center_crop=False, \n",
    "                                        random_flip=False,\n",
    "                                        train_batch_size=1,\n",
    "                                        val_batch_size=1,\n",
    "                                        num_workers=0,)\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "# Precompute the latents for the training set\n",
    "# train_latents = precompute_latents(vae, text_encoder, train_dataloader, train=True)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# # show the first 10 images in the dataloader\n",
    "# for i, batch in enumerate(train_dataloader):\n",
    "#     if i == 10:\n",
    "#         break\n",
    "#     plt.imshow(batch['pixel_values'][0].permute(1, 2, 0).cpu().numpy())\n",
    "#     plt.show()\n",
    "train_latents = precompute_latents(vae, text_encoder, train_dataloader, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 1024])\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "string = \"DRUSEN\"\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"text_encoder\").to(\"cuda\")\n",
    "\n",
    "\n",
    "# inputs = tokenizer(\n",
    "#     string, max_length=4 , padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "# )\n",
    "\n",
    "inputs = tokenizer(\n",
    "    string, max_length=tokenizer.model_max_length , padding=\"do_not_pad\", truncation=True, return_tensors=\"pt\"\n",
    ").input_ids\n",
    "\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Encode the string using the CLIPTextModel\n",
    "with torch.no_grad():\n",
    "    encoded = text_encoder(inputs)[0]\n",
    "\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"text_encoder\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 output = text_encoder(**<span style=\"color: #808000; text-decoration-color: #808000\">\"NORMAL\"</span>)                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(output.shape)                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>CLIPTextModel object argument after ** must be a mapping, not str\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 output = text_encoder(**\u001b[33m\"\u001b[0m\u001b[33mNORMAL\u001b[0m\u001b[33m\"\u001b[0m)                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[96mprint\u001b[0m(output.shape)                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0mCLIPTextModel object argument after ** must be a mapping, not str\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "string = \"NORMAL\"\n",
    "inputs = tokenizer(\n",
    "    string, max_length=tokenizer.model_max_length , padding=\"do_not_pad\", truncation=True, return_tensors=\"pt\"\n",
    ").input_ids\n",
    "\n",
    "output = text_encoder(inputs)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
