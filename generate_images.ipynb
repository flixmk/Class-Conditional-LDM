{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.0+cu118 with CUDA 1108 (you have 1.13.0+cu116)\n",
      "    Python  3.8.16 (you have 3.8.10)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "2023-05-26 10:20:57.870079: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-26 10:20:58.018085: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-26 10:20:58.516770: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n",
      "2023-05-26 10:20:58.516831: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n",
      "2023-05-26 10:20:58.516834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import shutil\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lightning.pytorch import LightningModule\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "\n",
    "from modules.evaluator import Evaluator\n",
    "\n",
    "from cleanfid import fid\n",
    "import timm\n",
    "import shutil\n",
    "import wandb\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\"/home/flix/epoch1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for class CNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:49<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for class DME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:50<00:00,  4.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for class DRUSEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:50<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for class NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:50<00:00,  4.43s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_images = 25\n",
    "class_prompts = [\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1-base\",\n",
    "    unet=unet,\n",
    "    vae=AutoencoderKL.from_pretrained(\"flix-k/custom_model_parts\", subfolder=\"vae_trained_kl\"),\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    "    )\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "pipeline.to(device)\n",
    "num_gpus = 1\n",
    "images_per_gpu = num_images // num_gpus\n",
    "save_path = \"./synth_data_GS4/\"\n",
    "for class_prompt in class_prompts:\n",
    "    \n",
    "    print(f\"Generating images for class {class_prompt}\")\n",
    "    if save_path is not None:\n",
    "        save_path_class = save_path + f\"/{class_prompt}\"\n",
    "        isExist = os.path.exists(save_path_class) \n",
    "        if not isExist:\n",
    "            os.makedirs(save_path_class)\n",
    "    for it in tqdm(range(images_per_gpu)):\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            images = pipeline(\n",
    "                prompt = class_prompt,\n",
    "                height = 512,\n",
    "                width = 512,\n",
    "                num_inference_steps = 25,\n",
    "                guidance_scale = 4,\n",
    "                negative_prompt = None,\n",
    "                num_images_per_prompt = 1,\n",
    "                ).images\n",
    "            for idx, image in enumerate(images):\n",
    "                id_num = idx + (it * 1)\n",
    "                id = str(id_num).zfill(len(str(num_images)))\n",
    "                image.save(f\"{save_path_class}/{class_prompt}-({id}).jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./synth_data_GS4/\"\n",
    "\n",
    "os.makedirs(f\"./{folder}/ALL\", exist_ok=True)\n",
    "\n",
    "for c in [\"NORMAL\", \"CNV\", \"DRUSEN\", \"DME\"]:\n",
    "    for f in os.listdir(f\"./{folder}/{c}\"):\n",
    "        if \".jpg\" in f:\n",
    "            shutil.copy(f\"./{folder}/{c}/{f}\", f\"./synth_data/ALL/{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute FID between two folders\n",
      "Found 100 images in the folder ./synth_data/ALL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID ALL : 100%|██████████| 4/4 [00:01<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images in the folder /home/flix/Documents/hf_datasets/OCT-datasetv3/val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID val : 100%|██████████| 125/125 [00:11<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.998051014705993\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import timm\n",
    "from cleanfid import fid\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = timm.create_model('inception_v3', pretrained=True, num_classes=4).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/flix/Documents/DeepFlix/generative/diffusion/evaluation/models/finetuned_best.pt\"))\n",
    "model.eval()\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "\n",
    "score = fid.compute_fid(\n",
    "    \"./synth_data/ALL\", \n",
    "    \"/home/flix/Documents/hf_datasets/OCT-datasetv3/val\", \n",
    "    mode=\"clean\",\n",
    "    custom_feat_extractor=model,\n",
    ")\n",
    "print(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FID: 5.590877777494846"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
